# Introduction

This project has been created by Automatic Unit test team (AutomatedUnitTest team  <AutomatedUnitTest@aurea.com>). Tests are created with C# Test Generator tool which is imprved by 30-40 CAs over many weeks. Provided tests are blackbox and graybox testing.

# Project Structure

For each one of projects in the project section we have a test project name ProjectName.AUT.Tests
- AutomaticUnitTests
    - Contains all the bulk or automatic unit test projects. (Only additional files from AUT team). Located at root\AUT\*.*
	- AUT sln located at root\EPMLive.AUT.Tests.sln
	
- Projects (existing projects )
- Readme
    - Readme.md
	
# Maintainability of Tests

Tests are not being maintained manually or by hand. Tests will be artificially generated by C# TestGenerator tool (v0.2) and added by the AUT team. Maintainability comes from the C# TestGenerator tool, as a team we don't modify the code by hand. If any test is failing due to future change, please get back to us, we will remove that test and generate a new ones using the tool.

For future maintenance, please get back to the developer or send email to "AutomatedUnitTest team <AutomatedUnitTest@aurea.com>".

# Business Value and Coverage gain

We provide automated and meaningful tests within the context of the source code file. Consequently, if a project is integrated with a database or many others things and have larger domain scope. Then it would hard for the AI at this moment to get the whole context (in future probably it will have that capability). AUT tests isolates and single method or properties and tries to test it in a simple manner to gain coverage. Mostly focuses on getters/setters, constructors, fields. Consequently, our tests don't have much business logic, however, business value in terms of coverage gain.

Note : *Handcrafted Unit Test (HUT) team should provide appropriate business values with actual business logic.*

# Coverage Report Tool and verification

AUT team usages Visual studio code coverage tool and all the reports are provided based on that tool. There is always a difference in CI due to tool difference. Every pull-request contains an initial and final screen-shot. By running all tests coverage from visual studio it should yield the same number as the screen-shot. Plus, team follow the independent coverage. For example, one team did 10K and we did 12K it doesn't mean it is going to sum up and show as 22K. Tests will be measured independently.

# Why automatic unit tests

Well, we can fake coverage gain by (http://bit.ly/2GiLlx3) or we can really do it automatically by our team and our tools.

 # Code Quality

Our code quality is better than to existing products like EvoSuit or DiffBlue. Our codes tend to follow AAA: Arrange, Act and Assert format. Our codes have C# standard naming conventions, however, if the targeted source file doesn't have appropriate naming then it would fail. However, most of the part it follows standard naming conventions from MSDN.
  
# Coverage Gain Improvements

Our team always strive for coverage improvements, each week we have 5-10 CAs from crossover as a participant to improve the C#TestGenerator tools. As merging and reviewing the codes takes some, as a result, releases takes some time too. Our new version will have MS Fakes and MOQ, test quality will be far better. In the next iteration.

# Service Level Agreement (SLA)

Our team seeks a strict 24 hours SLA. Any pull-request created should be reviewed by 24 hours and if no issues then should be approved and merged by the SLA time period. In case, SLA is not honored, it goes through VP and to the upper management. As it is very strict and so far we follow this SLA with every product and team we work with.
  
# Coverage Calculation
We mostly follow [LOC](http://bit.ly/2sDGL3z) metrics.
Note: ***Sometimes there is possiblity of negative percentage, due to adding new references.***

*For example, we are testing one class which is using another project as reference, however, we haven't added any tests to that project yet. In that case it will come as negative percetage for the time being. However, when we add tests to that reference project or complete whole project. It will have positive percentage gain when all are done.*

# Test Adapters
 
 We use NUnit and Nunit 3.10.x Adapter for running and verifying the tests. Additionally, we use Autofixture and Moq.

# Review Criteria

AUT tests are created using automated tool and there is no need to check generated codes line by line. For this reason, the PCA should verify build, no failing tests, coverage gain report using visual studio to verify the provided screenshot numbers.
  - For starter, team should provide 5-6 tests to PCA to review and understand how it is going to build in CI.

Review should be rejected only in these case : 
  - If there are double empty lines in the code, files are missing from solution which is causing build fail
  - Coverage is not matching with the screenshot (it needs to be huges difference from the provided sceenshot and the Visual Studio number in PCA's machine (NOT in CI)).
  - Tests numbers are not matching in large numbers (over 200 tests).

Review should not be rejected in these cases: 
  - Build failing in CI due to integration and it was passing in locally. It should be informed by comment to check the issue or investigate together.
  - Tests are failing in CI, however, passing locally (which can be verified by the provided screenshot that there is no failing tests from our end). It should be informed by comment to check the issue or investigate together.

# Working IC Contacts

|Name | Email |Skype |Slack|
|--|--|--|--|
|Mohammad Abdul Kadir|Mohammad.kadir@aurea.com|mak.rony1 |mohammad.kadir|